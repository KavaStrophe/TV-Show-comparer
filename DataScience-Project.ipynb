{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datascience project\n",
    "## Goal :\n",
    "The goal of this project is to create graph of characters from transcripts of popular Tv shows from https://transcripts.foreverdreaming.org.\n",
    "Theses graph will be merge per seasons and compared to the others graph in order to find correlations with their caracteristics.\n",
    "\n",
    "## How will it works ?\n",
    "First, we will get the data : we will download the transcripts for each tv show and store it into HTML files. Next, we will analyze each of them and generate one graph per episode and merge theses graphs to get a graph for each season. We will finally compare all the graphs with their caracteristics and try to find some characters pattern.\n",
    "\n",
    "## Get the data :\n",
    "So we will request https://transcripts.foreverdreaming.org and get the links of the tv shows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "urlWebsite = \"https://transcripts.foreverdreaming.org\";\n",
    "#We will define a fake agent to contourn the protection\n",
    "agent = {\"User-Agent\":'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'}\n",
    "#Interval between two http requests :\n",
    "interval = 1;\n",
    "\n",
    "#Important functions :\n",
    "\n",
    "#Return the soup of the link\n",
    "def get_soup(url):\n",
    "    print(get_now_datetime_str() + \" -> request : \" + url);\n",
    "    agent = {\"User-Agent\":'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'}\n",
    "    page = requests.get(url, headers=agent)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    return soup;\n",
    "\n",
    "#Return the soup of a local file\n",
    "def get_moms_soup(path):\n",
    "    print(get_now_datetime_str() + \" -> file : \" + path);\n",
    "    page = BeautifulSoup(open(path, encoding='utf8'), \"html.parser\", from_encoding=\"utf-8\")\n",
    "    return page;\n",
    "\n",
    "\n",
    "#Initialize a void pickle\n",
    "def init_pickle(path):\n",
    "    write_pickle(path, []);\n",
    "\n",
    "#Write a pickle\n",
    "def write_pickle(path, array_pickle):\n",
    "    pickle_out = open(path,\"wb\");\n",
    "    pickle.dump(array_pickle, pickle_out);\n",
    "    pickle_out.close();\n",
    "\n",
    "#Read a pickle\n",
    "def read_pickle(path, type_data=\"list\"):\n",
    "    file_path = Path(path);\n",
    "    if(file_path.exists()):\n",
    "        pickle_in = open(path,\"rb\");\n",
    "        data = pickle.load(pickle_in);\n",
    "        pickle_in.close();\n",
    "        return data;\n",
    "    else:\n",
    "        if(type_data == \"dict\"):\n",
    "            return {};\n",
    "        return [];\n",
    "    \n",
    "#Clean a string\n",
    "def clean_str(dirty_str):\n",
    "    cleaned_str = dirty_str.rstrip().lstrip();\n",
    "    return re.sub('[^a-zA-Z0-9 \\n\\.]', \"\", cleaned_str);\n",
    "    \n",
    "\n",
    "#Find the next page or return False if there is no next page\n",
    "def find_next_page(soup):\n",
    "    pagination = soup.findAll('b', attrs={'class':'pagination'})\n",
    "    if(len(pagination) == 0):\n",
    "        return False;\n",
    "    other_pages = pagination[0].findAll('a')\n",
    "    for page in other_pages:\n",
    "        if(page.getText() == \"Â»\"):\n",
    "            return page.get('href');\n",
    "    return False;\n",
    "\n",
    "#Transform the soup into a list of link to episode transcript\n",
    "def html_to_url_list_episode(soup):\n",
    "    link_list = [];\n",
    "    flag_info = False #True if the rows of info are passed.\n",
    "    table = soup.findAll('table', attrs={'class' : 'tablebg'});\n",
    "    rows = table[0].findAll('tr');\n",
    "    for row in rows:\n",
    "        if(not(flag_info)):\n",
    "            row_episode = row.findAll('b', attrs={'class' : 'gensmall'});\n",
    "            if(len(row_episode) != 0 and row_episode[0].getText() == \"Episode\"):\n",
    "                flag_info = True;\n",
    "        else:\n",
    "            line = row.findAll('a')[0];\n",
    "            row_link = urlWebsite + line.get('href')[1:];\n",
    "            name =  clean_str(line.getText());\n",
    "            row_name = name;\n",
    "            link_list.append([row_name, row_link]);\n",
    "    return link_list;\n",
    "\n",
    "#Return a string containing the current time\n",
    "def get_now_datetime_str():\n",
    "    now = datetime.now();\n",
    "    return now.strftime(\"%H:%M:%S\");\n",
    "\n",
    "#make a dir if not exist\n",
    "def mk_dir_ifn_exist(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path);\n",
    "   \n",
    "    \n",
    "#Loop the requesting of the page until it succeed\n",
    "def req_until_death(url):\n",
    "    time.sleep(interval);\n",
    "    agent = {\"User-Agent\":'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'};\n",
    "    request_done = False;\n",
    "    while(not(request_done)):\n",
    "        try:\n",
    "            print(get_now_datetime_str() + \" -> request : \" + url);\n",
    "            page = requests.get(url, headers=agent, timeout=10);\n",
    "            if(type(page) == \"ReadTimeout\" ):\n",
    "                continue;\n",
    "            else:\n",
    "                request_done = True;\n",
    "                soup = BeautifulSoup(page.content);\n",
    "            return soup;\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(get_now_datetime_str() + \" -> failure, retry !\");\n",
    "            time.sleep(60);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create important folders\n",
    "mk_dir_ifn_exist(\"img\");\n",
    "mk_dir_ifn_exist(\"data\");\n",
    "mk_dir_ifn_exist(\"pickle\");\n",
    "mk_dir_ifn_exist(\"pickle/series\");\n",
    "mk_dir_ifn_exist(\"pickle/graphs\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Get the index of all the tv shows\n",
    "WARNING : Do not execute the following code if you already have the following files : \n",
    "- pickle/tv_shows.pickle\n",
    "- pickle/tv_show_list.pickle \n",
    "- pickle/list_made.pickle !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make our soup\n",
    "soup = req_until_death(urlWebsite);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we have to find all the <a> link with the css class \"subforum\"\n",
    "tv_shows = {};\n",
    "tv_shows_list = [];\n",
    "for link in soup.findAll('a', attrs={\"class\" : \"subforum\"}):\n",
    "    name =  clean_str(link.getText());\n",
    "    tv_shows[name] = urlWebsite + link.get('href')[1:];\n",
    "    tv_shows_list.append(name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pickle(\"pickle/tv_shows.pickle\", tv_shows); #Contain a dict with the tv show and their link to the index of their transcript\n",
    "write_pickle(\"pickle/tv_shows_list.pickle\", tv_shows_list); #Contain a list of the tv_show\n",
    "\n",
    "for name in tv_shows_list:\n",
    "    if not os.path.exists(\"data/\" + name):\n",
    "        os.mkdir(\"data/\" + name);\n",
    "    if not os.path.exists(\"pickle/\" + name):\n",
    "        os.mkdir(\"pickle/\" + name);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF THE WARNING  \n",
    "Let's load the list of our tv shows and the dict that contains the link to the transcript's listing pages :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Create the index\n",
    "Now, we will create an index per tv show, each containing all the links to all his episodes !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_shows = read_pickle(\"pickle/tv_shows.pickle\");\n",
    "tv_shows_list = read_pickle(\"pickle/tv_shows_list.pickle\");\n",
    "list_series_already_indexed = read_pickle(\"pickle/list_series_already_indexed.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for serie in tv_shows_list:\n",
    "    print(get_now_datetime_str() + \" : \" + serie)\n",
    "    if(serie in list_series_already_indexed and read_pickle(\"pickle/\" + serie + \"/episode_list.pickle\") != []):\n",
    "        continue;\n",
    "        \n",
    "    else:\n",
    "        url = tv_shows[serie];\n",
    "        all_episode = [];\n",
    "        next_page_exist = True;\n",
    "        while(next_page_exist): #If there is another index page, get it !\n",
    "            soup = req_until_death(url);\n",
    "            next_page = find_next_page(soup);\n",
    "            if(next_page == False):\n",
    "                next_page_exist = False;\n",
    "            else:\n",
    "                url = urlWebsite + next_page[1:];\n",
    "            all_episode = all_episode + html_to_url_list_episode(soup);\n",
    "            \n",
    "        write_pickle(\"pickle/\" + serie + \"/episode_list.pickle\", all_episode);\n",
    "        \n",
    "    list_series_already_indexed.append(serie);\n",
    "    write_pickle(\"pickle/list_series_already_indexed.pickle\", list_series_already_indexed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Download transcripts\n",
    "Now, we will download all the transcript, it will have an HTML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_transcript_page(soup, name, path):\n",
    "    body = soup.findAll('div', attrs={'class':'postbody'})[0]\n",
    "    final_path = path + \"/\" + name + \".html\";\n",
    "    with open(final_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(body.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the pickle file \"already dl\" for each serie based on the HTML files saved in case where the pickle save had an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for serie in tv_shows_list:\n",
    "    print(get_now_datetime_str() + \" : \" + serie);\n",
    "    episodes = read_pickle(\"pickle/\" + serie + \"/episode_list.pickle\");\n",
    "    episode_already_dl = [];\n",
    "    for episode in episodes:\n",
    "        name = episode[0];\n",
    "        path = \"data/\" + serie + \"/\" + name + \".html\";\n",
    "        file_path = Path(path);\n",
    "        if(file_path.exists()):\n",
    "            print(get_now_datetime_str() + \" -> OK for : \" + name);\n",
    "            episode_already_dl.append(episode[0])\n",
    "            write_pickle(\"pickle/\" + serie + \"/episode_already_dl.pickle\",episode_already_dl);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download each HTML file containing the transcript of the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for serie in tv_shows_list:\n",
    "    print(get_now_datetime_str() + \" : \" + serie);\n",
    "    episodes = read_pickle(\"pickle/\" + serie + \"/episode_list.pickle\");\n",
    "    episode_already_dl = read_pickle(\"pickle/\" + serie + \"/episode_already_dl.pickle\");\n",
    "    \n",
    "    for episode in episodes:\n",
    "        if(episode[0] in episode_already_dl):\n",
    "            continue;\n",
    "        \n",
    "        episode_already_dl.append(episode[0])\n",
    "        name = episode[0];\n",
    "        url = episode[1];\n",
    "        print(get_now_datetime_str() + \" -> Reading : \" + episode[0]);\n",
    "        soup = req_until_death(url);\n",
    "        dl_transcript_page(soup, name, \"data/\" + serie);\n",
    "        write_pickle(\"pickle/\" + serie + \"/episode_already_dl.pickle\",episode_already_dl);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Building our data\n",
    "First we will extract the name of the characters and building their relations. Next, we will clean our data by excluding the outliers (that are probably false positive result of the character detection). We will build our graph and take some metrics to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract characters and relations from the html :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean a string\n",
    "def clean_string(string_to_clean):\n",
    "    string_to_clean = string_to_clean.replace('\\n', ' ');\n",
    "    string_to_clean = string_to_clean.strip();\n",
    "    string_to_clean = string_to_clean.lstrip();\n",
    "    string_to_clean = string_to_clean.rstrip();\n",
    "    return string_to_clean;\n",
    "\n",
    "#Return all the words identified as name of a character\n",
    "def id_characters(sentence):\n",
    "    name_list = [];\n",
    "    doc = nlp(sentence)\n",
    "    for ent in doc.ents:\n",
    "        if(ent.label_ == \"PERSON\"): #Get all the words identified as character's name\n",
    "            name = ent.text.lower();\n",
    "            name_list.append(clean_string(name));\n",
    "    return name_list;\n",
    "    \n",
    "#Extract characters and their relations from an html file\n",
    "def extract_charac_of_episode(file_path):\n",
    "    soup = get_moms_soup(file_path);\n",
    "    lines_html = soup.findAll(\"p\");\n",
    "    characters_list = {};\n",
    "    relations_list = [];\n",
    "    \n",
    "    for dirty_line in lines_html:\n",
    "        line = dirty_line.get_text();\n",
    "        line = clean_string(line);\n",
    "        characs = id_characters(line);\n",
    "        \n",
    "        for i in range(0, len(characs)):\n",
    "            #Create or increase the weight of the characters\n",
    "            if(not(characs[i] in characters_list)):\n",
    "                characters_list[characs[i]] = 1;\n",
    "            else:\n",
    "                characters_list[characs[i]] = characters_list[characs[i]] + 1;\n",
    "        \n",
    "            if(i+1 != len(characs)):\n",
    "                for j in range(i+1, len(characs)):\n",
    "                    next_charac = False; #The current character is the last one detected\n",
    "                else:\n",
    "                    next_charac = characs[i+1];\n",
    "                relation_found = False;\n",
    "                #Create or increase the weight of the relations\n",
    "                for x in range(0, len(relations_list)):\n",
    "                    if((characs[i] == relations_list[x][0] and next_charac == relations_list[x][1]) or (characs[i] == relations_list[x][1] and next_charac == relations_list[x][0])):\n",
    "                        relation_found = True;\n",
    "                        relations_list[x][2] = relations_list[x][2] + 1;\n",
    "                        break;\n",
    "                if(not(relation_found)):\n",
    "                    relations_list.append([characs[i], next_charac, 1])\n",
    "                    \n",
    "    return characters_list, relations_list;\n",
    "\n",
    "def merge_characs_serie(final_array, add_array):\n",
    "    for charac in add_array:\n",
    "        if(not(charac in final_array)):\n",
    "            final_array[charac] = 1;\n",
    "        else:\n",
    "            final_array[charac] = final_array[charac] + add_array[charac];\n",
    "        \n",
    "    return final_array;\n",
    "\n",
    "def merge_relations_serie(final_array, add_array):\n",
    "    for relation in add_array:\n",
    "        charac_1 = relation[0];\n",
    "        charac_2 = relation[1];\n",
    "        weight = relation[2];\n",
    "        \n",
    "        relation_found = False;\n",
    "        for i in range(0, len(final_array)):\n",
    "            final_relation = final_array[i];\n",
    "            if((charac_1 == final_relation[1] and charac_2 == final_relation[0]) or charac_1 == final_relation[0] and charac_2 == final_relation[1]):\n",
    "                final_array[i][2] = final_array[i][2] + weight;\n",
    "                relation_found = True;\n",
    "                break;\n",
    "        \n",
    "        if(not(relation_found)):\n",
    "            final_array.append([charac_1, charac_2, weight]);\n",
    "            \n",
    "    return final_array;\n",
    "\n",
    "#Extract all the characters and their relations from a serie (each episode one by one and merge the results)\n",
    "def extract_charac_of_serie(serie):\n",
    "    print(get_now_datetime_str() + \" -> extract begin : \" + serie);\n",
    "    list_episodes = read_pickle('pickle/' + serie + \"/episode_already_dl.pickle\");\n",
    "    final_characters = {};\n",
    "    final_relations = [];\n",
    "    \n",
    "    for episode in list_episodes:\n",
    "        path_episode = \"data/\" + serie + \"/\" + episode + '.html';\n",
    "        temp_chara, temp_rela = extract_charac_of_episode(path_episode);\n",
    "        final_characters = merge_characs_serie(final_characters, temp_chara);\n",
    "        final_relations = merge_relations_serie(final_relations, temp_rela);\n",
    "    \n",
    "    print(get_now_datetime_str() + \" -> extract end\");\n",
    "    return final_characters, final_relations;\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the characters and their relations from each tv show\n",
    "series = read_pickle('pickle/list_series_already_indexed.pickle')\n",
    "for serie in series:\n",
    "    if(read_pickle('pickle/' + serie + '/relations.pickle') != [] or read_pickle('pickle/' + serie + '/characters.pickle') != []):\n",
    "        continue;\n",
    "        \n",
    "    characters, relations = extract_charac_of_serie(serie);\n",
    "    write_pickle('pickle/' + serie + '/characters.pickle', characters);\n",
    "    write_pickle('pickle/' + serie + '/relations.pickle', relations);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our basic data, we have to clean them by remove the lowest outliers. Not the highest because they are probably the main characters.\n",
    "We will also remove the characters only mentionned one time and their relations because they are probably errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_per_characters(relations, characters):\n",
    "    count_relation_per_charac = {};\n",
    "    for name in characters:\n",
    "        count_relation_per_charac[name] = 0;\n",
    "\n",
    "    for relation in relations:\n",
    "        name_1 = None;\n",
    "        name_2 = None;\n",
    "        if(relation[0] in characters):\n",
    "            name_1 = relation[0];\n",
    "        if(relation[1] in characters):\n",
    "            name_2 = relation[1];\n",
    "\n",
    "        if(not(name_1 == None) and not(name_2 == None)):\n",
    "                count_relation_per_charac[name_1] = count_relation_per_charac[name_1] + 1;\n",
    "                count_relation_per_charac[name_2] = count_relation_per_charac[name_2] + 1;\n",
    "    \n",
    "    return count_relation_per_charac;\n",
    "    \n",
    "def clean_relations(dirty_relations, characters):\n",
    "    cleaned_relations = [];\n",
    "    for relation in dirty_relations:\n",
    "        if(relation[0] in characters and relation[1] in characters):\n",
    "            cleaned_relations.append(relation);\n",
    "    return cleaned_relations;\n",
    "\n",
    "def suppress_no_edge(characters, relations):\n",
    "    count_relation_per_charac = relation_per_characters(relations, characters);\n",
    "    new_characters = {};\n",
    "    for charac in characters:\n",
    "        if(count_relation_per_charac[charac]  > 0):\n",
    "            new_characters[charac] = characters[charac];\n",
    "    return new_characters;\n",
    "\n",
    "def suppress_one_and_less_occurence(characters):\n",
    "    new_characters = {};\n",
    "    for charac in characters:\n",
    "         if(characters[charac] > 1):\n",
    "            new_characters[charac] = characters[charac];\n",
    "    return new_characters;\n",
    "\n",
    "def keep_only_main_characters(characters):\n",
    "    if(characters != {}):\n",
    "        weights_characs = [];\n",
    "        for charac in characters:\n",
    "            weights_characs.append(characters[charac]);\n",
    "        sorted(weights_characs);\n",
    "\n",
    "        q1, q3= np.percentile(weights_characs,[25,75]);\n",
    "        iqr = q3 - q1;\n",
    "        lower_bound = q3 + iqr;\n",
    "        new_characs = {};\n",
    "        for charac in characters:\n",
    "            if(not(characters[charac] < lower_bound)):\n",
    "                new_characs[charac] = characters[charac];\n",
    "        characters = new_characs;\n",
    "    return characters\n",
    "\n",
    "def merge_same_characters(characters, relations):\n",
    "    replaced_names = [];\n",
    "    while(True):\n",
    "        replaced_character = False;\n",
    "\n",
    "        for name in characters:\n",
    "            for new_name in characters:\n",
    "                if((name in new_name) and (len(name) < len(new_name)) and (not(name in replaced_names))): #If one of the two name contain the other and is bigger\n",
    "                    replaced_character = True;\n",
    "                    weight = characters[new_name] + characters[name];\n",
    "                    characters[new_name] = weight;\n",
    "                    replaced_names.append(name);\n",
    "\n",
    "                    #TODO : Replace relations by a Hashmap based on the names of the two characters\n",
    "                    for relation in relations:  #Update the names in the relations\n",
    "                        if(relation[0] == name):\n",
    "                            relation[0] = new_name;\n",
    "                        if(relation[1] == name):\n",
    "                            relation[1] = new_name;\n",
    "\n",
    "        if(not(replaced_character)):\n",
    "            break;\n",
    "\n",
    "    for name in replaced_names:   \n",
    "        del characters[name];\n",
    "        \n",
    "    return characters, relations;\n",
    "\n",
    "def remove_relations_between_same_character(relations):\n",
    "    relations_new = [];\n",
    "    for relation in relations:\n",
    "        if(relation[0] != relation[1]):\n",
    "            relations_new.append(relation);\n",
    "    return relations_new;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(serie):\n",
    "    print(get_now_datetime_str() + \" -> cleaning : \" + serie);\n",
    "    relations = read_pickle('pickle/' + serie + '/relations.pickle');\n",
    "    characters = read_pickle('pickle/' + serie + '/characters.pickle');\n",
    "    \n",
    "    if(characters != {}):\n",
    "        #Check and replace if you found a more longest is declared for the same character\n",
    "        characters, relations = merge_same_characters(characters, relations);\n",
    "        relations = remove_relations_between_same_character(relations);\n",
    "            \n",
    "        #We suppress the nodes without any occurence or just one and the nodes without any edge\n",
    "        characters = suppress_one_and_less_occurence(characters);\n",
    "        relations = clean_relations(relations, characters);\n",
    "        characters = suppress_no_edge(characters, relations);\n",
    "        \n",
    "        #Now, we remove all the characters that cannot be considered as primary using Q3 + IQR :\n",
    "        characters = keep_only_main_characters(characters);\n",
    "        relations = clean_relations(relations, characters);\n",
    "        characters = suppress_no_edge(characters, relations);\n",
    "                    \n",
    "    write_pickle('pickle/' + serie + '/characters_cleaned.pickle', characters);\n",
    "    write_pickle('pickle/' + serie + '/relations_cleaned.pickle', relations);\n",
    "    return characters, relations;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = read_pickle('pickle/list_series_already_indexed.pickle')\n",
    "for serie in series:\n",
    "    if(read_pickle('pickle/' + serie + '/relations.pickle') == [] and read_pickle('pickle/' + serie + '/characters.pickle') == []):\n",
    "        continue;\n",
    "    characs, relations = clean_data(serie);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we now will create our graphs, one per serie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = read_pickle('pickle/list_series_already_indexed.pickle')\n",
    "graphs_complete = [];\n",
    "for serie in series:    \n",
    "    graph_serie = nx.Graph();\n",
    "    relations = read_pickle('pickle/' + serie + '/relations_cleaned.pickle');\n",
    "    characters = read_pickle('pickle/' + serie + '/characters_cleaned.pickle');\n",
    "    if(len(characters) != 0):\n",
    "        for character in characters:\n",
    "            graph_serie.add_node(character, weight=characters[character]);\n",
    "        for relation in relations:\n",
    "            graph_serie.add_edge(relation[0], relation[1], weight=relation[2]);\n",
    "        write_pickle('pickle/' + serie + '/graph.pickle', graph_serie)\n",
    "        if not(serie in graphs_complete):\n",
    "            graphs_complete.append(serie); #allow us to remove the series where we don't find any characters after our filters\n",
    "\n",
    "write_pickle('pickle/graph_serie.pickle', graphs_complete);\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will analyze theses graph as if they was social network graph. In this case, the main metrics are metrics of connections, distributions and segmentations :\n",
    "- Connections :\n",
    "    - Homophily (assortativity)\n",
    "    - Multiplexity (average degree of connectivity)\n",
    "    - Network Closure (transivitive closure)\n",
    "- Distributions :\n",
    "    - Centrality (degree, closeness, etc.)\n",
    "    - Density\n",
    "    - Structural holes\n",
    "- Segmentation\n",
    "    - Clustering coefficient\n",
    "    - Cohesion\n",
    "    \n",
    "So, we will first save our graph in picture and next analyze their caracteristics.  \n",
    "In addition to this, we will get the genre of each serie on IMDB and its rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_dict(dict_to_avg):\n",
    "    dict_total = 0;\n",
    "    for key in dict_to_avg:\n",
    "        dict_total = dict_total + dict_to_avg[key];\n",
    "    return (dict_total  / len(dict_to_avg));\n",
    "\n",
    "def min_dict(dict_to_min):\n",
    "    min_val = None;\n",
    "    for key in dict_to_min:\n",
    "        val = dict_to_min[key];\n",
    "        if(min_val == None):\n",
    "            min_val = val;\n",
    "        elif(val < min_val):\n",
    "            min_val = val;\n",
    "    return min_val;\n",
    "    \n",
    "def max_dict(dict_to_max):\n",
    "    max_val = None;\n",
    "    for key in dict_to_max:\n",
    "        val = dict_to_max[key];\n",
    "        if(max_val == None):\n",
    "            max_val = val;\n",
    "        elif(val > max_val):\n",
    "            max_val = val;\n",
    "    return max_val;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = read_pickle('pickle/graph_serie.pickle') #Now, we will only use the list of the tv show we created a graph with\n",
    "for serie in series:\n",
    "    print(get_now_datetime_str() + \" -> graph compute : \" + serie);\n",
    "    G = read_pickle('pickle/' + serie + '/graph.pickle')\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_title(serie, fontsize=10)\n",
    "\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos, node_size=1500, node_color='black', font_size=8, font_weight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"./img/graph_\" + serie + \".png\", format=\"PNG\")\n",
    "    plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = read_pickle('pickle/graph_serie.pickle');\n",
    "series_carac = {};\n",
    "for serie in series:\n",
    "    print(get_now_datetime_str() + \" -> graph compute : \" + serie);\n",
    "    graph = read_pickle('pickle/' + serie + '/graph.pickle')\n",
    "    graph_caracs = {};\n",
    "    \n",
    "    #Computation of thebetweeness of the graph\n",
    "    betweeness = nx.betweenness_centrality(graph, weight='weight');\n",
    "    graph_caracs['avg_betweeness'] = avg_dict(betweeness);\n",
    "    graph_caracs['min_betweeness'] = min_dict(betweeness);\n",
    "    graph_caracs['max_betweeness'] = max_dict(betweeness);\n",
    "    #Computation of the degree of connectity of the graph's nodes\n",
    "    deg_connect = nx.average_degree_connectivity(graph, weight='weight');\n",
    "    graph_caracs['avg_deg_connect'] = avg_dict(deg_connect);\n",
    "    graph_caracs['min_deg_connect'] = min_dict(deg_connect);\n",
    "    graph_caracs['max_deg_connect'] = max_dict(deg_connect);\n",
    "    #Computation of the degree centrality of the nodes of the graph\n",
    "    cent_degree = nx.degree_centrality(graph);\n",
    "    graph_caracs[\"avg_centrality_degree\"] = avg_dict(cent_degree);\n",
    "    graph_caracs[\"min_centrality_degree\"] = min_dict(cent_degree);\n",
    "    graph_caracs[\"max_centrality_degree\"] = max_dict(cent_degree);\n",
    "    #Computation of the centrality closeness\n",
    "    cent_close = nx.closeness_centrality(graph);\n",
    "    graph_caracs[\"avg_centrality_closeness\"] = avg_dict(cent_close);\n",
    "    graph_caracs[\"min_centrality_closeness\"] = min_dict(cent_close);\n",
    "    graph_caracs[\"max_centrality_closeness\"] = max_dict(cent_close\n",
    "    #Computation of the graph density\n",
    "    graph_caracs[\"density\"] = nx.density(graph);\n",
    "    #Computation of the structure hole of the nodes\n",
    "    struct_hole = nx.constraint(graph, weight=\"weight\");\n",
    "    graph_caracs[\"avg_struct_holes\"] = avg_dict(struct_hole);\n",
    "    graph_caracs[\"min_struct_holes\"] = min_dict(struct_hole);\n",
    "    graph_caracs[\"max_struct_holes\"] = max_dict(struct_hole);\n",
    "    #Computation of the clustering coeff\n",
    "    clus_coef = nx.clustering(graph, weight='weight');\n",
    "    graph_caracs[\"avg_clus_coef\"] = nx.average_clustering(graph, weight='weight');\n",
    "    graph_caracs[\"min_clus_coef\"] = min_dict(clus_coef);\n",
    "    graph_caracs[\"max_clus_coef\"] = max_dict(clus_coef);\n",
    "    \n",
    "    write_pickle('pickle/' + serie + '/graph_caracs.pickle', graph_caracs);\n",
    "    series_carac[serie] = graph_caracs;\n",
    "    \n",
    "write_pickle('pickle/all_graph_caracs.pickle', series_carac);\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the caracteristics of our graphs, we will collect some additionnal informations about them. So we will use IMDB to collect the genre and the rate of each tv show !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RepresentsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = read_pickle('pickle/graph_serie.pickle');\n",
    "imdb_info = {};\n",
    "url_site = \"https://www.imdb.com\";\n",
    "\n",
    "categories_per_show = read_pickle('pickle/metadata_per_show.pickle', 'dict');\n",
    "    \n",
    "for serie in series:\n",
    "    print(get_now_datetime_str() + \" -> Searching for : \" + serie);\n",
    "    if(serie in categories_per_show):\n",
    "        continue;\n",
    "    name_serie_url = serie.replace(' ', '+')\n",
    "    url = \"https://www.imdb.com/search/title/?title=\" + name_serie_url + \"&title_type=tv_series\" #We use the edvanced research of the website.\n",
    "    \n",
    "    genres = None;\n",
    "    rate = None;\n",
    "    \n",
    "    soup = get_soup(url);\n",
    "    box = soup.findAll(\"div\", {\"class\":\"lister-item-content\"});\n",
    "    if(len(box) == 0): #If no result with the advanced research, we use the general research and take the first film returned.\n",
    "        print(get_now_datetime_str() + \" -> No result, I will use the alternative research...\");\n",
    "        url = \"https://www.imdb.com/find?ref_=nv_sr_fn&q=\" + name_serie_url + \"&s=all\"\n",
    "        soup = get_soup(url);\n",
    "        box = None;\n",
    "        \n",
    "        for result_list in soup.findAll('div', {\"class\" : \"findSection\"}):\n",
    "            title = result_list.findAll('h3')[0]\n",
    "            if(\"Titles\" in title.text):\n",
    "                box = result_list.findAll(\"td\", {\"class\":\"result_text\"})[0];\n",
    "                    \n",
    "        link = box.findAll(\"a\")[0]['href'];\n",
    "        \n",
    "        soup = get_soup(url_site + link);\n",
    "        sub_box = soup.findAll(\"div\", {\"class\" : \"subtext\"})[0];\n",
    "        links = sub_box.findAll(\"href\");\n",
    "        for link in links:\n",
    "            href = link['href'];\n",
    "            if(\"genre\" in href):\n",
    "                genre.append(clean_str(link.text));\n",
    "                \n",
    "        rate_soup = soup.findAll('span', {'itemprop': 'ratingValue'});\n",
    "        if(len(rate_soup) == 0):\n",
    "            rate = None;\n",
    "        else:\n",
    "            rate = clean_str(rate_soup[0].text);\n",
    "        \n",
    "    else:\n",
    "        box = box[0];\n",
    "        genres = [];\n",
    "        genres_brut = (box.findAll(\"span\", {\"class\" : \"genre\"})[0].text).split(\", \");\n",
    "        for genre in genres_brut:\n",
    "            genres.append(clean_str(genre));\n",
    "        rate = clean_str(box.findAll(\"strong\")[0].text);\n",
    "\n",
    "    categories_per_show[serie] = {};\n",
    "    categories_per_show[serie][\"rate\"] = rate;\n",
    "    categories_per_show[serie][\"genres\"] = genres;\n",
    "    write_pickle('pickle/metadata_per_show.pickle', categories_per_show);\n",
    "print(categories_per_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will join the tv show with their genres. In order to simplify our classfifier, for one tv show with multiple genres, we will make multiple tv show similar with one different genre each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = read_pickle('pickle/graph_serie.pickle');\n",
    "categories_per_show = read_pickle('pickle/metadata_per_show.pickle', 'dict');\n",
    "database_tv_show = [];\n",
    "tv_show_in_database = []\n",
    "\n",
    "for serie in series:\n",
    "    graph_caracs = read_pickle('pickle/' + serie + '/graph_caracs.pickle');\n",
    "    if(categories_per_show[serie]['genres'] == None or categories_per_show[serie]['rate'] == None):\n",
    "        continue;\n",
    "    genres = categories_per_show[serie]['genres'];\n",
    "    new_row = {};\n",
    "    \n",
    "    new_row[\"name\"] = serie;\n",
    "    new_row['rate'] = categories_per_show[serie]['rate'];\n",
    "    \n",
    "    for carac in graph_caracs:\n",
    "        new_row[carac] = graph_caracs[carac];\n",
    "    \n",
    "    new_database = [];\n",
    "    \n",
    "    for genre in genres:\n",
    "        final_new_row = {}\n",
    "        final_new_row[\"genre\"] = genre;\n",
    "        for carac in new_row:\n",
    "            final_new_row[carac] = new_row[carac]\n",
    "        new_database.append(final_new_row);\n",
    "        \n",
    "    database_tv_show = database_tv_show + new_database;\n",
    "    \n",
    "write_pickle('pickle/tv_show_in_database.pickle', tv_show_in_database);\n",
    "write_pickle('pickle/database_tv_show.pickle', database_tv_show);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_array = read_pickle('pickle/database_tv_show.pickle');\n",
    "database_df = pd.DataFrame.from_dict(database_array, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_genre = [];\n",
    "for genre in database_df['genre']:\n",
    "    if(not(genre in list_genre)):\n",
    "        list_genre.append(genre);\n",
    "print(list_genre);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will see which values we could remove to simplify our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = database_df.drop(['genre', 'name'], axis=1)\n",
    "score2 = StandardScaler().fit_transform(df)\n",
    "\n",
    "pca = PCA()\n",
    "principalComponents = pca.fit_transform(score2)\n",
    "\n",
    "pca_list = [];\n",
    "for i in range(0, len(df.columns)):\n",
    "    pca_list.append(\"pc\" + str(i));\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = pca_list)\n",
    "finalDf = pd.concat([principalDf, database_df[['genre']]], axis = 1)\n",
    "\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "\n",
    "genres = ['Mystery', 'Comedy', 'Drama', 'Thriller', 'Crime', 'Music', 'Romance', 'Family', 'SciFi', 'History', 'Horror', 'Fantasy', 'TalkShow', 'War']\n",
    "colors = [];\n",
    "for genre in genres:\n",
    "    colors.append(np.random.rand(3,1))\n",
    "\n",
    "\n",
    "for genre, color in zip(genres, colors):\n",
    "    indicesToKeep = finalDf['genre'] == genre\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'pc1']\n",
    "               , finalDf.loc[indicesToKeep, 'pc2']\n",
    "               , c = color.ravel()\n",
    "               , s = 50)\n",
    "ax.legend(genres)\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "y_pos = np.arange(len(pca_list))\n",
    "performance = [10,8,6,4,2,1]\n",
    "\n",
    "plt.bar(y_pos, pca.explained_variance_ratio_, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, pca_list)\n",
    "plt.ylabel('Weight')\n",
    "plt.title('PCs')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "score_df=  pd.DataFrame(pca.components_,columns=df.columns,index = pca_list)[:2]\n",
    "print('\\n#########################################\\n')\n",
    "print(\"SCORES :\")\n",
    "print('\\n#########################################\\n')\n",
    "print (score_df)\n",
    "\n",
    "columns_to_keep = [];\n",
    "column_score = {};\n",
    "for column in score_df:\n",
    "    pc1_value = score_df[column][0];\n",
    "    pc2_value = score_df[column][1];\n",
    "    if(pc1_value < 0):\n",
    "        pc1_value = pc1_value*-1;\n",
    "    if(pc2_value < 0):\n",
    "        pc2_value = pc2_value*-1;\n",
    "        \n",
    "    column_score[column] = pc2_value + pc1_value;\n",
    "    \n",
    "    if((score_df[column][0] < -0.3 or score_df[column][0] > 0.3) or (score_df[column][1] > 0.3 or score_df[column][1] < -0.3)):\n",
    "        columns_to_keep.append(column)\n",
    "        \n",
    "print(column_score)\n",
    "print('\\n#########################################\\n')\n",
    "print(\"COLUMNS TO KEEP : \")\n",
    "print('\\n#########################################\\n')\n",
    "print(columns_to_keep);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we can see on the bar chart, the main data are contained in the first two components. The columns that influe the most on these data are (We only keep the column with more than 0.3 of score) :\n",
    "- For PC0 :\n",
    "    - Average centrality closeness\n",
    "    - Average centrality degree\n",
    "    - Density\n",
    "    - Min centrality closeness\n",
    "    - Min centrality degree\n",
    "    - Min structure holes\n",
    "\n",
    "- For PC1 :\n",
    "    - Average betweeness\n",
    "    - Average degree of connection\n",
    "    - Max betweeness\n",
    "    - Max degree of connection\n",
    "    - Min degree of connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df = database_df[columns_to_keep + ['genre', 'name']]\n",
    "write_pickle('pickle/new_database.pickle', database_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use a logistic regression as a classifier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import collections;\n",
    "\n",
    "def partition (list_in, n):\n",
    "    perc = int(n*len(list_in))\n",
    "    random.shuffle(list_in)\n",
    "    test_list = [];\n",
    "    train_list = [];\n",
    "    for i in range(0, perc):\n",
    "        test_list.append(list_in[i]);\n",
    "    for i in range(perc, len(list_in)):\n",
    "        train_list.append(list_in[i]);\n",
    "    return test_list, train_list\n",
    "\n",
    "def detect_error(test_list, test_result, database, clf):\n",
    "    database_df = read_pickle('pickle/new_database.pickle');\n",
    "    results = [];\n",
    "    no_predicted = {};\n",
    "    total_genres = {};\n",
    "    for index, test in test_list.iterrows():\n",
    "        \n",
    "        y_predicted = clf.predict_proba(test_list.loc[[index]]);\n",
    "        name = test_result.loc[[index]]['name'].values[0]\n",
    "        genres = database_df['genre'].loc[database_df['name'] == name].tolist()\n",
    "        \n",
    "        class_for_proba = dict(zip(clf.classes_, y_predicted[0]));\n",
    "        most_proba_genres = sorted(class_for_proba, key=class_for_proba.get, reverse=True)[:len(genres)]\n",
    "        \n",
    "        good_answer = 0;\n",
    "        for genre in genres:\n",
    "            if(genre in most_proba_genres):\n",
    "                good_answer = good_answer + 1;\n",
    "            else:\n",
    "                if(genre in no_predicted):\n",
    "                    no_predicted[genre] = no_predicted[genre] + 1;\n",
    "                else:\n",
    "                    no_predicted[genre] = 1;\n",
    "                    \n",
    "            if(not(genre in total_genres)):\n",
    "                total_genres[genre] = 1;\n",
    "            else:\n",
    "                total_genres[genre] = total_genres[genre] + 1;\n",
    "                \n",
    "        results.append(good_answer/len(genres) * 100);\n",
    "    \n",
    "    perc_result = np.mean(results);\n",
    "    no_predicted = dict(sorted(no_predicted.items(), key=lambda kv: kv[1], reverse=True))\n",
    "    return perc_result, no_predicted, total_genres;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(train_size):\n",
    "    database_df = read_pickle('pickle/new_database.pickle');\n",
    "    series = read_pickle('pickle/graph_serie.pickle');\n",
    "    part_test, part_train = partition(series, train_size);\n",
    "\n",
    "    y = database_df[['genre', 'name']];\n",
    "    x = database_df.drop(['genre', 'name'], axis=1);\n",
    "\n",
    "    train = database_df[database_df['name'].isin(part_train)]\n",
    "    test = database_df[database_df['name'].isin(part_test)]\n",
    "\n",
    "    x_train = train.drop(['genre', 'name'], axis=1);\n",
    "    x_test =  test.drop(['genre', 'name'], axis=1);\n",
    "    y_train =  train[['genre', 'name']]\n",
    "    y_test =  test[['genre', 'name']];\n",
    "\n",
    "    clf = LogisticRegression(max_iter=5000, random_state=0, solver='lbfgs', multi_class='multinomial').fit(x_train, y_train['genre'])\n",
    "\n",
    "    test_result, no_pred, total_genre = detect_error(x_test, y_test, database_df, clf)\n",
    "    return test_result, no_pred, total_genre, clf, test, train;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little test to see the impact of the split of the data into training and test dataset will impact our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(read_pickle('pickle/size_train_test_results.pickle')) != 0):\n",
    "    train_size_list, results_for_train_size = read_pickle('pickle/size_train_test_results.pickle')\n",
    "else:\n",
    "    train_size_list = [];\n",
    "    results_for_train_size = [];\n",
    "    \n",
    "for i in range(5, 100, 5):\n",
    "    train_size = i/100;\n",
    "    if(train_size in train_size_list):\n",
    "        continue;\n",
    "    else:\n",
    "        print(get_now_datetime_str() + \" -> Test for train size = \" + str(train_size));\n",
    "    run_test_results = [];\n",
    "    nb_iter = 200\n",
    "    for j in range(0, nb_iter):\n",
    "        result_score, _ , _ ,_, _, _= run_test(train_size);\n",
    "        run_test_results.append(result_score);\n",
    "        \n",
    "    results_for_train_size.append(np.mean(run_test_results));\n",
    "    train_size_list.append(train_size);\n",
    "    write_pickle('pickle/size_train_test_results.pickle', [train_size_list, results_for_train_size])\n",
    "    print(get_now_datetime_str() + \" -> Average success = \" + str(np.mean(run_test_results)));\n",
    "    \n",
    "write_pickle('pickle/size_train_test_results.pickle', [train_size_list, results_for_train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_list, results_for_train_size = read_pickle('pickle/size_train_test_results.pickle')\n",
    "\n",
    "plt.bar(train_size_list, results_for_train_size, align='center', alpha=0.5)\n",
    "plt.xticks(train_size_list)\n",
    "plt.ylabel('Train size')\n",
    "plt.title('Positive answer')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will test to build our classifier with the best training set to maximise our results :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = run_test(0.2);\n",
    "for i in range(0, 500):\n",
    "    new_model = run_test(0.2);\n",
    "    if(new_model[0] > best_model[0]):\n",
    "        best_model = new_model;\n",
    "        \n",
    "write_pickle('pickle/best_model.pickle', best_model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model[0])\n",
    "for genre in best_model[1]:\n",
    "    print(\"Error for \" + genre + \" : \" + str(round(best_model[1][genre] / best_model[2][genre] * 100, 2)) + \"% - \" + str(best_model[1][genre]) + ' on ' + str(best_model[2][genre]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we finally have a classifier based on the logistic regression algorithm that get 63% of success on our testing dataset. Considering that each TV show have often between 2 and 3 genres and that we have 17 different genres, its an interesting result. However, some genres need more data like SciFi, Animation, Family, Biography, Thriller, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
