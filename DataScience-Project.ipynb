{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datascience project\n",
    "## Goal :\n",
    "The goal of this project is to create graph of characters from transcripts of popular Tv shows from https://transcripts.foreverdreaming.org.\n",
    "Theses graph will be merge per seasons and compared to the others graph in order to find correlations with their caracteristics.\n",
    "\n",
    "## How will it works ?\n",
    "First, we will get the data : we will download the transcripts for each tv show and store it into HTML files. Next, we will analyze each of them and generate one graph per episode and merge theses graphs to get a graph for each season. We will finally compare all the graphs with their caracteristics and try to find some characters pattern.\n",
    "\n",
    "## Get the data :\n",
    "So we will request https://transcripts.foreverdreaming.org and get the links of the tv shows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "urlWebsite = \"https://transcripts.foreverdreaming.org\";\n",
    "#We will define a fake agent to contourn the protection\n",
    "agent = {\"User-Agent\":'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'}\n",
    "#Interval between two http requests :\n",
    "interval = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important functions :\n",
    "\n",
    "#Return the soup of the link\n",
    "def get_soup(url):\n",
    "    print(get_now_datetime_str() + \" -> request : \" + url);\n",
    "    agent = {\"User-Agent\":'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'}\n",
    "    page = requests.get(url, headers=agent)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    return soup;\n",
    "\n",
    "#Return the soup of a local file\n",
    "def get_moms_soup(path):\n",
    "    print(get_now_datetime_str() + \" -> file : \" + path);\n",
    "    page = BeautifulSoup(open(path, encoding='utf8'), \"html.parser\", from_encoding=\"utf-8\")\n",
    "    return page;\n",
    "\n",
    "\n",
    "#Initialize a void pickle\n",
    "def init_pickle(path):\n",
    "    write_pickle(path, []);\n",
    "\n",
    "#Write a pickle\n",
    "def write_pickle(path, array_pickle):\n",
    "    pickle_out = open(path,\"wb\");\n",
    "    pickle.dump(array_pickle, pickle_out);\n",
    "    pickle_out.close();\n",
    "\n",
    "#Read a pickle\n",
    "def read_pickle(path):\n",
    "    file_path = Path(path);\n",
    "    if(file_path.exists()):\n",
    "        pickle_in = open(path,\"rb\");\n",
    "        data = pickle.load(pickle_in);\n",
    "        pickle_in.close();\n",
    "        return data;\n",
    "    else:\n",
    "        return [];\n",
    "    \n",
    "#Clean a string\n",
    "def clean_str(dirty_str):\n",
    "    return re.sub('[^a-zA-Z0-9 \\n\\.]', \"\", dirty_str);\n",
    "\n",
    "#Find the next page or return False if there is no next page\n",
    "def find_next_page(soup):\n",
    "    pagination = soup.findAll('b', attrs={'class':'pagination'})\n",
    "    if(len(pagination) == 0):\n",
    "        return False;\n",
    "    other_pages = pagination[0].findAll('a')\n",
    "    for page in other_pages:\n",
    "        if(page.getText() == \"Â»\"):\n",
    "            return page.get('href');\n",
    "    return False;\n",
    "\n",
    "#Transform the soup into a list of link to episode transcript\n",
    "def html_to_url_list_episode(soup):\n",
    "    link_list = [];\n",
    "    flag_info = False #True if the rows of info are passed.\n",
    "    table = soup.findAll('table', attrs={'class' : 'tablebg'});\n",
    "    rows = table[0].findAll('tr');\n",
    "    for row in rows:\n",
    "        if(not(flag_info)):\n",
    "            row_episode = row.findAll('b', attrs={'class' : 'gensmall'});\n",
    "            if(len(row_episode) != 0 and row_episode[0].getText() == \"Episode\"):\n",
    "                flag_info = True;\n",
    "        else:\n",
    "            line = row.findAll('a')[0];\n",
    "            row_link = urlWebsite + line.get('href')[1:];\n",
    "            name =  clean_str(line.getText());\n",
    "            row_name = name;\n",
    "            link_list.append([row_name, row_link]);\n",
    "    return link_list;\n",
    "\n",
    "#Return a string containing the current time\n",
    "def get_now_datetime_str():\n",
    "    now = datetime.now();\n",
    "    return now.strftime(\"%H:%M:%S\");\n",
    "\n",
    "#make a dir if not exist\n",
    "def mk_dir_ifn_exist(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path);\n",
    "   \n",
    "    \n",
    "#Loop the requesting of the page until it succeed\n",
    "def req_until_death(url):\n",
    "    time.sleep(interval);\n",
    "    agent = {\"User-Agent\":'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'};\n",
    "    request_done = False;\n",
    "    while(not(request_done)):\n",
    "        try:\n",
    "            print(get_now_datetime_str() + \" -> request : \" + url);\n",
    "            page = requests.get(url, headers=agent, timeout=10);\n",
    "            if(type(page) == \"ReadTimeout\" ):\n",
    "                continue;\n",
    "            else:\n",
    "                request_done = True;\n",
    "                soup = BeautifulSoup(page.content);\n",
    "            return soup;\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(get_now_datetime_str() + \" -> failure, retry !\");\n",
    "            time.sleep(60);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create important folders\n",
    "mk_dir_ifn_exist(\"data\");\n",
    "mk_dir_ifn_exist(\"pickle\");\n",
    "mk_dir_ifn_exist(\"pickle/series\");\n",
    "mk_dir_ifn_exist(\"pickle/graphs\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Get the index of all the tv shows\n",
    "WARNING : Do not execute the following code if you already have the following files : \n",
    "- pickle/tv_shows.pickle\n",
    "- pickle/tv_show_list.pickle \n",
    "- pickle/list_made.pickle !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make our soup\n",
    "soup = req_until_death(urlWebsite);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we have to find all the <a> link with the css class \"subforum\"\n",
    "tv_shows = {};\n",
    "tv_shows_list = [];\n",
    "for link in soup.findAll('a', attrs={\"class\" : \"subforum\"}):\n",
    "    name =  clean_str(link.getText());\n",
    "    tv_shows[name] = urlWebsite + link.get('href')[1:];\n",
    "    tv_shows_list.append(name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pickle(\"pickle/tv_shows.pickle\", tv_shows); #Contain a dict with the tv show and their link to the index of their transcript\n",
    "write_pickle(\"pickle/tv_shows_list.pickle\", tv_shows_list); #Contain a list of the tv_show\n",
    "\n",
    "for name in tv_shows_list:\n",
    "    if not os.path.exists(\"data/\" + name):\n",
    "        os.mkdir(\"data/\" + name);\n",
    "    if not os.path.exists(\"pickle/\" + name):\n",
    "        os.mkdir(\"pickle/\" + name);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF THE WARNING  \n",
    "Let's load the list of our tv shows and the dict that contains the link to the transcript's listing pages :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Create the index\n",
    "Now, we will create an index per tv show, each containing all the links to all his episodes !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_shows = read_pickle(\"pickle/tv_shows.pickle\");\n",
    "tv_shows_list = read_pickle(\"pickle/tv_shows_list.pickle\");\n",
    "list_series_already_indexed = read_pickle(\"pickle/list_series_already_indexed.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for serie in tv_shows_list:\n",
    "    print(get_now_datetime_str() + \" : \" + serie)\n",
    "    if(serie in list_series_already_indexed and read_pickle(\"pickle/\" + serie + \"/episode_list.pickle\") != []):\n",
    "        continue;\n",
    "        \n",
    "    else:\n",
    "        url = tv_shows[serie];\n",
    "        all_episode = [];\n",
    "        next_page_exist = True;\n",
    "        while(next_page_exist): #If there is another index page, get it !\n",
    "            soup = req_until_death(url);\n",
    "            next_page = find_next_page(soup);\n",
    "            if(next_page == False):\n",
    "                next_page_exist = False;\n",
    "            else:\n",
    "                url = urlWebsite + next_page[1:];\n",
    "            all_episode = all_episode + html_to_url_list_episode(soup);\n",
    "            \n",
    "        write_pickle(\"pickle/\" + serie + \"/episode_list.pickle\", all_episode);\n",
    "        \n",
    "    list_series_already_indexed.append(serie);\n",
    "    write_pickle(\"pickle/list_series_already_indexed.pickle\", list_series_already_indexed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Download transcripts\n",
    "Now, we will download all the transcript, it will have an HTML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_transcript_page(soup, name, path):\n",
    "    body = soup.findAll('div', attrs={'class':'postbody'})[0]\n",
    "    final_path = path + \"/\" + name + \".html\";\n",
    "    with open(final_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(body.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the pickle file \"already dl\" for each serie based on the HTML files saved in case where the pickle save had an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for serie in tv_shows_list:\n",
    "    print(get_now_datetime_str() + \" : \" + serie);\n",
    "    episodes = read_pickle(\"pickle/\" + serie + \"/episode_list.pickle\");\n",
    "    episode_already_dl = [];\n",
    "    for episode in episodes:\n",
    "        name = episode[0];\n",
    "        path = \"data/\" + serie + \"/\" + name + \".html\";\n",
    "        file_path = Path(path);\n",
    "        if(file_path.exists()):\n",
    "            print(get_now_datetime_str() + \" -> OK for : \" + name);\n",
    "            episode_already_dl.append(episode[0])\n",
    "            write_pickle(\"pickle/\" + serie + \"/episode_already_dl.pickle\",episode_already_dl);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download each HTML file containing the transcript of the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for serie in tv_shows_list:\n",
    "    print(get_now_datetime_str() + \" : \" + serie);\n",
    "    episodes = read_pickle(\"pickle/\" + serie + \"/episode_list.pickle\");\n",
    "    episode_already_dl = read_pickle(\"pickle/\" + serie + \"/episode_already_dl.pickle\");\n",
    "    \n",
    "    for episode in episodes:\n",
    "        if(episode[0] in episode_already_dl):\n",
    "            continue;\n",
    "        \n",
    "        episode_already_dl.append(episode[0])\n",
    "        name = episode[0];\n",
    "        url = episode[1];\n",
    "        print(get_now_datetime_str() + \" -> Reading : \" + episode[0]);\n",
    "        soup = req_until_death(url);\n",
    "        dl_transcript_page(soup, name, \"data/\" + serie);\n",
    "        write_pickle(\"pickle/\" + serie + \"/episode_already_dl.pickle\",episode_already_dl);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Building our data\n",
    "First we will extract the name of the characters and building their relations. Next, we will clean our data by excluding the outliers (that are probably false positive result of the character detection). We will build our graph and take some metrics to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract characters and relations from the html :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean a string\n",
    "def clean_string(string_to_clean):\n",
    "    string_to_clean = string_to_clean.replace('\\n', ' ');\n",
    "    string_to_clean = string_to_clean.strip();\n",
    "    string_to_clean = string_to_clean.lstrip();\n",
    "    string_to_clean = string_to_clean.rstrip();\n",
    "    return string_to_clean;\n",
    "\n",
    "#Return all the words identified as name of a character\n",
    "def id_characters(sentence):\n",
    "    name_list = [];\n",
    "    doc = nlp(sentence)\n",
    "    for ent in doc.ents:\n",
    "        if(ent.label_ == \"PERSON\"): #Get all the words identified as character's name\n",
    "            name = ent.text.lower();\n",
    "            name_list.append(clean_string(name));\n",
    "    return name_list;\n",
    "    \n",
    "#Extract characters and their relations from an html file\n",
    "def extract_charac_of_episode(file_path):\n",
    "    soup = get_moms_soup(file_path);\n",
    "    lines_html = soup.findAll(\"p\");\n",
    "    characters_list = {};\n",
    "    relations_list = [];\n",
    "    \n",
    "    for dirty_line in lines_html:\n",
    "        line = dirty_line.get_text();\n",
    "        line = clean_string(line);\n",
    "        characs = id_characters(line);\n",
    "        \n",
    "        for i in range(0, len(characs)):\n",
    "            #Create or increase the weight of the characters\n",
    "            if(not(characs[i] in characters_list)):\n",
    "                characters_list[characs[i]] = 1;\n",
    "            else:\n",
    "                characters_list[characs[i]] = characters_list[characs[i]] + 1;\n",
    "        \n",
    "            if(i+1 != len(characs)):\n",
    "                for j in range(i+1, len(characs)):\n",
    "                    next_charac = False; #The current character is the last one detected\n",
    "                else:\n",
    "                    next_charac = characs[i+1];\n",
    "                relation_found = False;\n",
    "                #Create or increase the weight of the relations\n",
    "                for x in range(0, len(relations_list)):\n",
    "                    if((characs[i] == relations_list[x][0] and next_charac == relations_list[x][1]) or (characs[i] == relations_list[x][1] and next_charac == relations_list[x][0])):\n",
    "                        relation_found = True;\n",
    "                        relations_list[x][2] = relations_list[x][2] + 1;\n",
    "                        break;\n",
    "                if(not(relation_found)):\n",
    "                    relations_list.append([characs[i], next_charac, 1])\n",
    "                    \n",
    "    return characters_list, relations_list;\n",
    "\n",
    "def merge_characs_serie(final_array, add_array):\n",
    "    for charac in add_array:\n",
    "        if(not(charac in final_array)):\n",
    "            final_array[charac] = 1;\n",
    "        else:\n",
    "            final_array[charac] = final_array[charac] + add_array[charac];\n",
    "        \n",
    "    return final_array;\n",
    "\n",
    "def merge_relations_serie(final_array, add_array):\n",
    "    for relation in add_array:\n",
    "        charac_1 = relation[0];\n",
    "        charac_2 = relation[1];\n",
    "        weight = relation[2];\n",
    "        \n",
    "        relation_found = False;\n",
    "        for i in range(0, len(final_array)):\n",
    "            final_relation = final_array[i];\n",
    "            if((charac_1 == final_relation[1] and charac_2 == final_relation[0]) or charac_1 == final_relation[0] and charac_2 == final_relation[1]):\n",
    "                final_array[i][2] = final_array[i][2] + weight;\n",
    "                relation_found = True;\n",
    "                break;\n",
    "        \n",
    "        if(not(relation_found)):\n",
    "            final_array.append([charac_1, charac_2, weight]);\n",
    "            \n",
    "    return final_array;\n",
    "\n",
    "#Extract all the characters and their relations from a serie (each episode one by one and merge the results)\n",
    "def extract_charac_of_serie(serie):\n",
    "    print(get_now_datetime_str() + \" -> extract begin : \" + serie);\n",
    "    list_episodes = read_pickle('pickle/' + serie + \"/episode_already_dl.pickle\");\n",
    "    final_characters = {};\n",
    "    final_relations = [];\n",
    "    \n",
    "    for episode in list_episodes:\n",
    "        path_episode = \"data/\" + serie + \"/\" + episode + '.html';\n",
    "        temp_chara, temp_rela = extract_charac_of_episode(path_episode);\n",
    "        final_characters = merge_characs_serie(final_characters, temp_chara);\n",
    "        final_relations = merge_relations_serie(final_relations, temp_rela);\n",
    "    \n",
    "    print(get_now_datetime_str() + \" -> extract end\");\n",
    "    return final_characters, final_relations;\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the characters and their relations from each tv show\n",
    "series = read_pickle('pickle/list_series_already_indexed.pickle')\n",
    "for serie in series:\n",
    "    if(read_pickle('pickle/' + serie + '/relations.pickle') != [] or read_pickle('pickle/' + serie + '/characters.pickle') != []):\n",
    "        continue;\n",
    "        \n",
    "    characters, relations = extract_charac_of_serie(serie);\n",
    "    write_pickle('pickle/' + serie + '/characters.pickle', characters);\n",
    "    write_pickle('pickle/' + serie + '/relations.pickle', relations);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our basic data, we have to clean them by remove the lowest outliers. Not the highest because they are probably the main characters.\n",
    "We will also remove the characters only mentionned one time and their relations because they are probably errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def clean_data(serie):\n",
    "    print(get_now_datetime_str() + \" -> cleaning : \" + serie);\n",
    "    characters = {};\n",
    "    dirty_characters = [];\n",
    "    relations = [];\n",
    "    relations_extract = read_pickle('pickle/' + serie + '/relations.pickle');\n",
    "    characters_extract = read_pickle('pickle/' + serie + '/characters.pickle');\n",
    "    \n",
    "    if(characters_extract != {}):\n",
    "        #First supress the one-occurences\n",
    "        for charac in characters_extract:\n",
    "            if(characters_extract[charac] > 1):\n",
    "                characters[charac] = characters_extract[charac];\n",
    "            else:\n",
    "                dirty_characters.append(charac);\n",
    "\n",
    "        #Now, we remove the low outliers using IQR only with lower bound*1.5 :\n",
    "        if(characters != {}):\n",
    "            weights_characs = [];\n",
    "            for charac in characters:\n",
    "                weights_characs.append(characters[charac]);\n",
    "            sorted(weights_characs);\n",
    "            \n",
    "            q1, q3= np.percentile(weights_characs,[25,75]);\n",
    "            iqr = q3 - q1;\n",
    "            lower_bound = q1 -(1.5 * iqr);\n",
    "            new_characs = {};\n",
    "            for charac in characters:\n",
    "                if(characters[charac] < lower_bound):\n",
    "                    dirty_characters.append(charac);\n",
    "                else:\n",
    "                    new_characs[charac] = characters[charac];\n",
    "            characters = new_characs;\n",
    "            \n",
    "        #We remove the relations related to the removed characters\n",
    "        for relation in relations_extract:\n",
    "            if(relation[0] in dirty_characters or relation[1] in dirty_characters):\n",
    "                continue;\n",
    "            else:\n",
    "                relations.append(relation); \n",
    "        \n",
    "        #Check and replace if you found a more longest is declared for the same character\n",
    "        replaced_names = [];\n",
    "        while(True):\n",
    "            replaced_character = False;\n",
    "\n",
    "            for name in characters:\n",
    "                for new_name in characters:\n",
    "                    if((name in new_name) and (len(name) < len(new_name)) and (not(name in replaced_names))): #If one of the two name contain the other and is bigger\n",
    "                        replaced_character = True;\n",
    "                        weight = characters[new_name] + characters[name];\n",
    "                        characters[new_name] = weight;\n",
    "                        replaced_names.append(name);\n",
    "\n",
    "                        #TODO : Replace relations by a Hashmap based on the names of the two characters\n",
    "                        for relation in relations:  #Update the names in the relations\n",
    "                            if(relation[0] == charac):\n",
    "                                relation[0] = new_name;\n",
    "                            if(relation[1] == charac):\n",
    "                                relation[1] = new_name;\n",
    "            if(not(replaced_character)):\n",
    "                break;\n",
    "                \n",
    "        for name in replaced_names:   \n",
    "            del characters[name];\n",
    "                \n",
    "    write_pickle('pickle/' + serie + '/characters_cleaned.pickle', characters);\n",
    "    write_pickle('pickle/' + serie + '/relations_cleaned.pickle', relations);\n",
    "    return characters, relations;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = read_pickle('pickle/list_series_already_indexed.pickle')\n",
    "test = [];\n",
    "for serie in series:\n",
    "    if(read_pickle('pickle/' + serie + '/relations.pickle') == [] and read_pickle('pickle/' + serie + '/characters.pickle') == []):\n",
    "        continue;\n",
    "    characs, _ = clean_data(serie);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we now will create our graphs, one per serie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = read_pickle('pickle/list_series_already_indexed.pickle')\n",
    "for serie in series:    \n",
    "    graph_serie = nx.Graph();\n",
    "    relations = read_pickle('pickle/' + serie + '/relations_cleaned.pickle');\n",
    "    characters = read_pickle('pickle/' + serie + '/characters_cleaned.pickle');\n",
    "    for character in characters:\n",
    "        graph_serie.add_node(character, weight=characters[character]);\n",
    "    for relation in relations:\n",
    "        graph_serie.add_edge(relation[0], relation[1], weight=relation[2]);\n",
    "    write_pickle('pickle/' + serie + '/graph.pickle', graph_serie)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will analyze theses graph as if they was social network graph. In this case, the main metrics are metrics of connections, distributions and segmentations :\n",
    "- Connections :\n",
    "    - Homophily (assortativity)\n",
    "    - Multiplexity (average degree of connectivity)\n",
    "    - Network Closure (transivitive closure)\n",
    "- Distributions :\n",
    "    - Centrality (degree, closeness, etc.)\n",
    "    - Density\n",
    "    - Structural holes\n",
    "- Segmentation\n",
    "    - Clustering coefficient\n",
    "    - Cohesion\n",
    "    \n",
    "So, we will first save our graph in picture and next analyze their caracteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:16:38 -> graph compute : 12 Monkeys\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-d949c0e5b928>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumeric_assortativity_coefficient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'weight'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mgraph_caracs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"homophily\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumeric_assortativity_coefficient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'weight'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\networkx\\algorithms\\assortativity\\correlation.py\u001b[0m in \u001b[0;36mnumeric_assortativity_coefficient\u001b[1;34m(G, attribute, nodes)\u001b[0m\n\u001b[0;32m    230\u001b[0m            \u001b[0mPhysical\u001b[0m \u001b[0mReview\u001b[0m \u001b[0mE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m67\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;36m26126\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2003\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \"\"\"\n\u001b[1;32m--> 232\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumeric_mixing_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnumeric_ac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\networkx\\algorithms\\assortativity\\mixing.py\u001b[0m in \u001b[0;36mnumeric_mixing_matrix\u001b[1;34m(G, attribute, nodes, normalized)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m     \u001b[0mmapping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_to_numpy_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "series = read_pickle('pickle/list_series_already_indexed.pickle')\n",
    "for serie in series:\n",
    "    print(get_now_datetime_str() + \" -> graph compute : \" + serie);\n",
    "    graph = read_pickle('pickle/' + serie + '/graph.pickle')\n",
    "    graph_caracs = {};\n",
    "    \n",
    "    file_path = Path(\"./img/graph_\" + serie + \".png\");\n",
    "    if(not(file_path.exists())):\n",
    "        G = graph;\n",
    "        fig = plt.figure(figsize=(12,12))\n",
    "        ax = plt.subplot(111)\n",
    "        ax.set_title(serie, fontsize=10)\n",
    "\n",
    "        pos = nx.spring_layout(G)\n",
    "        nx.draw(G, pos, node_size=1500, node_color='black', font_size=8, font_weight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"./img/graph_\" + serie + \".png\", format=\"PNG\")\n",
    "        plt.close();\n",
    "    \n",
    "    print(nx.numeric_assortativity_coefficient(graph, 'weight'))\n",
    "    \n",
    "    \n",
    "    graph_caracs[\"homophily\"] = nx.numeric_assortativity_coefficient(graph, 'weight');\n",
    "    graph_caracs[\"multiplexy\"] = nx.average_degree_connectivity(graph, weight='weight');\n",
    "    graph_caracs[\"net_closure\"] = nx.transitive_closure(graph);\n",
    "    \n",
    "    graph_caracs[\"centrality_degree\"] = nx.degree_centrality(graph);\n",
    "    graph_caracs[\"centrality_closeness\"] = nx.closeness_centrality(graph);\n",
    "    graph_caracs[\"centrality_eigvect\"] = nx.eigenvector_centrality(graph, weight=\"weight\");\n",
    "    graph_caracs[\"density\"] = nx.density(graph);\n",
    "    graph_caracs[\"struct_holes\"] = nx.constraint(graph, weight=\"weight\");\n",
    "    \n",
    "    graph_caracs[\"avg_clus_coef\"] = nx.average_clustering(graph, weight='weight');\n",
    "    graph_caracs[\"min_clus_coef\"] = min(nx.clustering(graph, weight='weight'));\n",
    "    graph_caracs[\"max_clus_coef\"] = max(nx.clustering(graph, weight='weight'));\n",
    "    \n",
    "    graph_caracs['cohesion'] = nx.k_components(graph);\n",
    "    \n",
    "    \n",
    "    write_pickle('pickle/' + serie + '/graph_caracs.pickle', graph_caracs)\n",
    "    break;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
